version: '3.8'

services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: bitnami/kafka:3.3.1
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - ALLOW_PLAINTEXT_LISTENER=yes

  spark:
    image: bitnami/spark:3.3.1
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      # --- Delta Lake Configuration (Relying only on mounted JARs) ---
      # REMOVED: SPARK_JARS_PACKAGES=io.delta:delta-spark_2.12:3.3.0
      # REMOVED: SPARK_SUBMIT_OPTIONS=--packages io.delta:delta-spark_2.12:3.3.0
      # Explicitly add JARs from the mounted volume to the classpath
      - SPARK_DRIVER_CLASS_PATH=/opt/bitnami/spark/custom-jars/*:/opt/bitnami/spark/conf
      - SPARK_EXECUTOR_EXTRA_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      # Still need to explicitly enable Delta extensions and catalog
      - SPARK_SUBMIT_ARGS=--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      # --- End Delta Lake Configuration ---
    ports:
      - "8085:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master Port
    volumes:
      - ./data:/data
      - ./spark-config:/opt/bitnami/spark/conf
      - ./spark-jars:/opt/bitnami/spark/custom-jars # Your Delta JARs

  spark-worker:
    image: bitnami/spark:3.3.1
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      # --- Delta Lake Configuration (Relying only on mounted JARs) ---
      # REMOVED: SPARK_JARS_PACKAGES=io.delta:delta-spark_2.12:3.3.0
      # Explicitly add JARs from the mounted volume to the classpath
      - SPARK_EXECUTOR_EXTRA_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      # --- End Delta Lake Configuration ---
    depends_on:
      - spark
    ports:
      - "8081:8081" # Spark Worker Web UI
    volumes:
      - ./data:/data
      - ./spark-config:/opt/bitnami/spark/conf
      - ./spark-jars:/opt/bitnami/spark/custom-jars # Your Delta JARs

  jupyter:
    image: jupyter/pyspark-notebook:spark-3.3.1
    container_name: jupyter-notebook
    ports:
      - "8888:8888" # Jupyter Notebook UI
    environment:
      - SPARK_MASTER=spark://spark:7077
      - JUPYTER_ENABLE_LAB=yes
      # --- Delta Lake Configuration (Relying only on mounted JARs for PySpark) ---
      # REMOVED: --packages io.delta:delta-spark_2.12:3.3.0
      # Pass custom JAR path to PySpark
      - PYSPARK_SUBMIT_ARGS=--driver-class-path /home/jovyan/custom-jars/* --conf spark.executor.extraClassPath=/home/jovyan/custom-jars/* --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell
      # - PYSPARK_SUBMIT_ARGS=--driver-class-path /home/jovyan/custom-jars/* --conf spark.executor.extraClassPath=/home/jovyan/custom-jars/* --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell
      # --- End Delta Lake Configuration ---
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
      - ./spark-jars:/home/jovyan/custom-jars # Your Delta JARs
    depends_on:
      - spark
    working_dir: /home/jovyan/work

  postgres:
    image: postgres:13
    container_name: postgres
    restart: always
    environment:
      POSTGRES_DB: postgresDB
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data

  superset:
    image: apache/superset
    container_name: superset
    restart: always
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_LOAD_EXAMPLES=no
      - SUPERSET_SECRET_KEY=supersecretkey
      - SQLALCHEMY_DATABASE_URI=postgresql://admin:admin@postgres:5432/postgresDB
    depends_on:
      - postgres
    volumes:
      - ./superset:/app/superset_home
      - ./scripts:/scripts
    command: ["/bin/bash", "-c", "/scripts/superset_init.sh && superset run -h 0.0.0.0 -p 8088"]

  prefect-server:
    image: prefecthq/prefect:2-latest
    container_name: prefect-server
    command: prefect server start
    ports:
      - "4200:4200"
    volumes:
      - prefect-vol:/root/.prefect

  prefect-agent:
    image: prefecthq/prefect:2-latest
    container_name: prefect-agent
    depends_on:
      - prefect-server
    command: prefect agent start -q default
    volumes:
      - ./flows:/flows
      - prefect-vol:/root/.prefect
    working_dir: /flows

volumes:
  postgres-data:
  prefect-vol: